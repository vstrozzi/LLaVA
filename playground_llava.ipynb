{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "from llava.conversation import conv_templates\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def image_parser(image_file, sep=\",\"):\n",
    "    out = image_file.split(sep)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cluster/work/vogtlab/Group/vstrozzi/cache/models--openai--clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "### IMPORT: On Biomedcluster change .config under model_path to point towards correct vision_tower clip path\n",
    "device = \"cuda\"\n",
    "model_name = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_path = \"/cluster/work/vogtlab/Group/vstrozzi/cache/models--liuhaotian--llava-v1.5-7b/snapshots/4481d270cc22fd5c4d1bb5df129622006ccd9234/\"\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(model_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "Describe the image ASSISTANT:\n",
      "torch.Size([1, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/cluster/apps/vogtlab/users/vstrozzi/software/anaconda/envs/llava/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a man standing in front of a large white sign, which is likely a billboard. He is wearing a white shirt and appears to be the main focus of the scene. The man is positioned near the center of the image, with the billboard behind him.\n",
      "\n",
      "In the background, there are two other people, one on the left side and another on the right side of the image. A clock can be seen in the upper right corner of the scene, and a book is located near the center of the image. The overall atmosphere of the image suggests a public space or a commercial area.\n"
     ]
    }
   ],
   "source": [
    "# Params\n",
    "prompt = \"Describe the image\"\n",
    "image_file = \"images/catdog.png\"\n",
    "max_new_tokens = 512\n",
    "num_beams = 1 # numer of path of decision, less faster\n",
    "sep =  \",\"\n",
    "temperature = 0 # 0 lowest, det\n",
    "top_p = None\n",
    "images_embeds = True\n",
    "\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "# Making prompt in correct format\n",
    "if IMAGE_PLACEHOLDER in prompt:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        prompt = re.sub(IMAGE_PLACEHOLDER, image_token_se, prompt)\n",
    "    else:\n",
    "        prompt = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, prompt)\n",
    "else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        prompt = image_token_se + \"\\n\" + prompt\n",
    "    else:\n",
    "        prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "\n",
    "# Derive necessary conv\n",
    "if \"llama-2\" in model_name.lower():\n",
    "    conv_mode = \"llava_llama_2\"\n",
    "elif \"mistral\" in model_name.lower():\n",
    "    conv_mode = \"mistral_instruct\"\n",
    "elif \"v1.6-34b\" in model_name.lower():\n",
    "    conv_mode = \"chatml_direct\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "    conv_mode = \"llava_v1\"\n",
    "elif \"mpt\" in model_name.lower():\n",
    "    conv_mode = \"mpt\"\n",
    "else:\n",
    "    conv_mode = \"llava_v0\"\n",
    "\n",
    "if conv_mode is not None and conv_mode != conv_mode:\n",
    "    print(\n",
    "        \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
    "            conv_mode, conv_mode, conv_mode\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    conv_mode = conv_mode\n",
    "\n",
    "# Load conversation mode standard template \n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "print(prompt)\n",
    "\n",
    "# Load images from online or local\n",
    "image_files = image_parser(image_file, sep)\n",
    "images = load_images(image_files)\n",
    "image_sizes = [x.size for x in images]\n",
    "# Convert images to format b, 3, h, w (h = w) with resizing or padding\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)\n",
    "\n",
    "## HERE TO ADD MANUAL SPATIAL_FEATURES\n",
    "spatial_features = torch.randn((1, 576, 1024))# if want to manually edit features, we want b, nr_spat (576), d (1024)\n",
    "\n",
    "# Tokenize prompt\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "if images_embeds:\n",
    "    images_tensor = spatial_features.to(device, dtype=torch.float16)\n",
    "# Generate an answer by using full model LLava\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        image_sizes=image_sizes,\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        num_beams=num_beams,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "        images_embeds = images_embeds # If want to give images embeds already precomputed TODO: Only support 1 image\n",
    "\n",
    "    )\n",
    "\n",
    "# Print the output\n",
    "outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
